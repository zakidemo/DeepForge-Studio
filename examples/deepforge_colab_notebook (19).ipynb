{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepForge_Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ DeepForge Studio - AI Model Training Pipeline\\n",
        "\\n",
        "**Auto-generated Training Notebook**\\n",
        "\\n",
        "### âš¡ Quick Start:\\n",
        "1. **Enable GPU:** `Runtime â†’ Change runtime type â†’ GPU`\\n",
        "2. **Run this notebook:** `Runtime â†’ Run all`\\n",
        "3. **Add your data** where indicated in the code\\n",
        "4. **Download your trained model** using the last cell\\n",
        "\\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# DeepForge Studio - Exported Training Pipeline\n# Generated: 2025-12-15T21:31:55.035Z\n\nimport os, random\nimport numpy as np\n\ndef set_seed(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    try:\n        import tensorflow as tf\n        tf.random.set_seed(seed)\n    except Exception:\n        pass\n\nset_seed(42)\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import VGG16\n\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n# Load pretrained VGG16 model\nbase_model = VGG16(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet'\n)\n\n# Create the complete model\ninputs = tf.keras.Input(shape=(224, 224, 3))\n\n# Data augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.2),\n])\n\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\n\n# Default classification head\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(10, activation='softmax')(x)\n\n# Build the model\nmodel = tf.keras.Model(inputs, outputs)\n\n# Compile with different learning rates for fine-tuning\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Model summary\nprint(f\"Total layers: {len(model.layers)}\")\nprint(f\"Trainable layers: {sum([layer.trainable for layer in model.layers])}\")\nmodel.summary()\n\n# ============================\n# TRAINING PIPELINE (EDIT DATA PATH)\n# ============================\nimport tensorflow as tf\n\nDATA_DIR = \"path/to/your/image_dataset\"\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nEPOCHS = 50\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"categorical\",\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"categorical\",\n)\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n# Re-compile using UI-selected hyperparams (overrides template defaults safely)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n\nmodel.save(\"deepforge_model.keras\")\nprint(\"Saved model to deepforge_model.keras\")\n"
    }
  ]
}